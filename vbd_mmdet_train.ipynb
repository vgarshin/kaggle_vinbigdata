{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import time\n",
    "import json\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import Image, clear_output\n",
    "from collections import Counter\n",
    "from ensemble_boxes import *\n",
    "import copy\n",
    "import os.path as osp\n",
    "import mmcv\n",
    "import numpy as np\n",
    "from mmdet.datasets.builder import DATASETS\n",
    "from mmdet.datasets.custom import CustomDataset\n",
    "from mmcv import Config\n",
    "from mmdet.apis import set_random_seed\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 'v6'\n",
    "TEST = False\n",
    "PARAMS = {\n",
    "    'version': VER,\n",
    "    'folds': 7,\n",
    "    'val_fold': 0,\n",
    "    'img_size': 1024,\n",
    "    'batch_size': 4,\n",
    "    'epochs': 50,\n",
    "    'seed': 2020,\n",
    "    'sup': 'nms', # 'nms' or 'wbf'\n",
    "    'iou_th': .5,\n",
    "    'skip_box_th': .0001,\n",
    "    # 0\n",
    "    #'config': 'faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py',    \n",
    "    # 1\n",
    "    #'config': 'faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco.py',\n",
    "    #'checkpoint': 'mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth',\n",
    "    # 2\n",
    "    'config': 'vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py',\n",
    "    'checkpoint': 'vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth',\n",
    "    # 3\n",
    "    #'config': 'vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco.py',\n",
    "    #'checkpoint': 'vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth',\n",
    "    'comments': ''\n",
    "}\n",
    "DATA_PATH = '/u01/mrorange/vinbigdata/data'\n",
    "WRK_DIR = f'{DATA_PATH}/workmmd'\n",
    "IMGS_PATH = f'{DATA_PATH}/train_{PARAMS[\"img_size\"]}'\n",
    "MDLS_PATH = f'/u01/mrorange/vinbigdata/models_mmdet_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "    \n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_all(PARAMS['seed'])\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2color = [\n",
    "    [59,  238, 119], [222, 21,  229], [94,  49,  164], \n",
    "    [206, 221, 133], [117, 75,    3], [210, 224, 119], \n",
    "    [211, 176, 166], [63,  7,   197], [102, 65,   77], \n",
    "    [194, 134, 175], [209, 219,  50], [255, 44,   47], \n",
    "    [89,  125, 149], [110, 27,  100]\n",
    "]\n",
    "\n",
    "def plot_img(img, size=(18, 18), is_rgb=True, title='', cmap='gray'):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_imgs(imgs, cols=2, size=10, is_rgb=True, title='', cmap='gray', img_size=None):\n",
    "    rows = len(imgs) // cols + 1\n",
    "    fig = plt.figure(figsize=(cols * size, rows * size))\n",
    "    for i, img in enumerate(imgs):\n",
    "        if img_size is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "        fig.add_subplot(rows, cols, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    plt.suptitle(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "def draw_bbox(image, box, label, color, thickness=3):   \n",
    "    alpha = .1\n",
    "    alpha_box = .4\n",
    "    overlay_bbox = image.copy()\n",
    "    overlay_text = image.copy()\n",
    "    output = image.copy()\n",
    "    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, .6, 1)[0]\n",
    "    cv2.rectangle(overlay_bbox, \n",
    "                  (box[0], box[1]), \n",
    "                  (box[2], box[3]), \n",
    "                  color, -1)\n",
    "    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n",
    "    cv2.rectangle(overlay_text, \n",
    "                  (box[0], box[1] - 7 - text_height), \n",
    "                  (box[0] + text_width + 2, box[1]),\n",
    "                  (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay_text, alpha_box, output, 1 - alpha_box, 0, output)\n",
    "    cv2.rectangle(output, \n",
    "                  (box[0], box[1]), \n",
    "                  (box[2], box[3]),\n",
    "                  color, thickness)\n",
    "    cv2.putText(output, \n",
    "                label.upper(), \n",
    "                (box[0], box[1]-5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                .6, (255, 255, 255), 1, \n",
    "                cv2.LINE_AA)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST: \n",
    "    !mkdir {WRK_DIR}/checkpoints\n",
    "    !wget -c http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth \\\n",
    "          -O {WRK_DIR}/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth\n",
    "    !wget -c https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth \\\n",
    "        -O {WRK_DIR}/checkpoints/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth\n",
    "    !wget -c https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth \\\n",
    "        -O {WRK_DIR}/checkpoints/vfnet_r101_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-7729adb5.pth\n",
    "    \n",
    "    config = f'{WRK_DIR}/mmdetection/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'\n",
    "    checkpoint = f'{WRK_DIR}/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "    model = init_detector(config, checkpoint, device='cuda:0')\n",
    "    img = 'tomatokillers.jpg'\n",
    "    result = inference_detector(model, img)\n",
    "    show_result_pyplot(model, img, result, score_thr=.5, fig_size=(3, 2))\n",
    "else:\n",
    "    print('no test mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
    "train_df['img_path'] = train_df.apply(lambda row: f'{IMGS_PATH}/{row.image_id}.png', axis =1)\n",
    "print('train loaded:', train_df.shape)\n",
    "meta_df = pd.read_csv(f'{DATA_PATH}/train_meta_{PARAMS[\"img_size\"]}.csv')\n",
    "print('meta loaded:', meta_df.shape)\n",
    "train_df = pd.merge(train_df, meta_df, on='image_id')\n",
    "print('merged:', train_df.shape)\n",
    "train_df['x_min'] = train_df.apply(lambda row: PARAMS['img_size']*row.x_min/row.dim1, axis=1)\n",
    "train_df['y_min'] = train_df.apply(lambda row: PARAMS['img_size']*row.y_min/row.dim0, axis=1)\n",
    "train_df['x_max'] = train_df.apply(lambda row: PARAMS['img_size']*row.x_max/row.dim1, axis=1)\n",
    "train_df['y_max'] = train_df.apply(lambda row: PARAMS['img_size']*row.y_max/row.dim0, axis=1)\n",
    "train_df['width'] = train_df.apply(lambda row: row.x_max - row.x_min, axis=1)\n",
    "train_df['height'] = train_df.apply(lambda row: row.y_max - row.y_min, axis=1)\n",
    "train_df = train_df[train_df.class_id != 14].reset_index(drop = True)\n",
    "class_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\n",
    "classes = list(np.array(class_names)[np.argsort(class_ids)])\n",
    "classes = list(map(lambda x: str(x), classes))\n",
    "print('classes:', classes)\n",
    "\"\"\"\n",
    "train_df = train_df[(train_df.width >= 10) & \n",
    "                    (train_df.height >= 10) &\n",
    "                    (train_df.x_min >= 1) & \n",
    "                    (train_df.y_min >= 1) &\n",
    "                    (train_df.x_max <= PARAMS['img_size'] - 1) & \n",
    "                    (train_df.y_max <= PARAMS['img_size'] - 1)]\n",
    "train_df.reset_index(inplace=True)\n",
    "\"\"\"\n",
    "gkf  = GroupKFold(n_splits=PARAMS['folds'])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups=train_df.image_id.tolist())):\n",
    "    train_df.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{WRK_DIR}/train.txt', 'w') as file:\n",
    "    tr_ids = list(train_df[train_df['fold'] != 0].image_id.unique())\n",
    "    print('train:', len(tr_ids))\n",
    "    file.write('\\n'.join(tr_ids))\n",
    "with open(f'{WRK_DIR}/val.txt', 'w') as file:\n",
    "    val_ids = list(train_df[train_df['fold'] == 0].image_id.unique())\n",
    "    print('val:', len(val_ids))\n",
    "    file.write('\\n'.join(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DATASETS.register_module()\n",
    "class VBDDataset(CustomDataset):\n",
    "    CLASSES = classes.copy()\n",
    "    ANN_DF = train_df.copy()\n",
    "    def load_annotations(self, ann_file):\n",
    "        cat2label = {k: i for i, k in enumerate(self.CLASSES)}\n",
    "        image_list = mmcv.list_from_file(self.ann_file)\n",
    "        data_infos = []\n",
    "        for image_id in image_list:\n",
    "            img_anns = self.ANN_DF[self.ANN_DF.image_id == image_id]\n",
    "            filename = img_anns['img_path'].values[0]\n",
    "            data_info = dict(\n",
    "                filename=filename, \n",
    "                width=PARAMS['img_size'], \n",
    "                height=PARAMS['img_size']\n",
    "            )\n",
    "            #print('=========== BEFORE ===========')\n",
    "            #print('labels:', img_anns['class_id'].tolist())\n",
    "            #print('boxes:', img_anns[['x_min', 'y_min', 'x_max', 'y_max']])\n",
    "            boxes_list = []\n",
    "            scores_list = []\n",
    "            labels_list = []\n",
    "            boxes_img = []\n",
    "            labels_img = []\n",
    "            cls_ids = img_anns['class_id'].unique().tolist()\n",
    "            count_dict = Counter(img_anns['class_id'].tolist())\n",
    "            for cid in cls_ids:\n",
    "                if count_dict[cid] == 1:\n",
    "                    labels_img.append(cid)\n",
    "                    boxes_img.append(\n",
    "                        img_anns[\n",
    "                            img_anns.class_id == cid\n",
    "                        ][\n",
    "                            ['x_min', 'y_min', 'x_max', 'y_max']\n",
    "                        ].to_numpy().squeeze().tolist()\n",
    "                    )\n",
    "                else:\n",
    "                    cls_list =img_anns[img_anns.class_id == cid]['class_id'].tolist()\n",
    "                    #labels_list.append(cls_list)\n",
    "                    labels_list.extend(cls_list)\n",
    "                    bbox = img_anns[\n",
    "                        img_anns.class_id == cid\n",
    "                    ][\n",
    "                        ['x_min', 'y_min', 'x_max', 'y_max']\n",
    "                    ].to_numpy() / PARAMS['img_size']\n",
    "                    #boxes_list.append(bbox.tolist())\n",
    "                    #scores_list.append(np.ones(len(cls_list)).tolist())\n",
    "                    #weights.append(1)\n",
    "                    boxes_list.extend(bbox.tolist())\n",
    "                    scores_list.extend(np.ones(len(cls_list)).tolist())\n",
    "            #print('norm:', boxes_list)\n",
    "            #print('labels:', labels_list)\n",
    "            #print('scores:', scores_list)\n",
    "            if PARAMS['sup'] == 'nms':\n",
    "                boxes, scores, box_labels = nms(\n",
    "                    boxes=[boxes_list], \n",
    "                    scores=[scores_list], \n",
    "                    labels=[labels_list], \n",
    "                    #weights=weights,\n",
    "                    weights=None,\n",
    "                    iou_thr=PARAMS['iou_th']\n",
    "                )\n",
    "            elif PARAMS['sup'] == 'wbf':\n",
    "                boxes, scores, box_labels = weighted_boxes_fusion(\n",
    "                    boxes_list=[boxes_list], \n",
    "                    scores_list=[scores_list],\n",
    "                    labels_list=[labels_list], \n",
    "                    weights=None,\n",
    "                    iou_thr=PARAMS['iou_th'],\n",
    "                    skip_box_thr=PARAMS['skip_box_th']\n",
    "                )\n",
    "            else:\n",
    "                raise AttributeError('wrong supression param')\n",
    "            try:\n",
    "                boxes *= PARAMS['img_size']\n",
    "                boxes = boxes.tolist()\n",
    "                #print('back from norm:', boxes)\n",
    "                box_labels = box_labels.astype(int).tolist()\n",
    "                boxes.extend(boxes_img)\n",
    "                box_labels.extend(labels_img)\n",
    "                gt_labels = box_labels #img_anns['class_id'].tolist()\n",
    "                gt_bboxes = boxes #img_anns[['x_min', 'y_min', 'x_max', 'y_max']]\n",
    "            except:\n",
    "                gt_labels = labels_img\n",
    "                gt_bboxes = boxes_img\n",
    "            #print('=========== AFTER ===========')\n",
    "            #print('labels:', gt_labels)\n",
    "            #print('boxes:', gt_bboxes)\n",
    "            data_anno = dict(\n",
    "                bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n",
    "                labels=np.array(gt_labels, dtype=np.long)\n",
    "            )\n",
    "            data_info.update(ann=data_anno)\n",
    "            data_infos.append(data_info)\n",
    "        return data_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "train_transforms = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.RandomBrightness(limit=.2, p=1), \n",
    "        A.RandomContrast(limit=.2, p=1), \n",
    "        A.RandomGamma(p=1)\n",
    "    ], p=.5),\n",
    "    A.OneOf([\n",
    "        A.Blur(blur_limit=3, p=1),\n",
    "        A.MedianBlur(blur_limit=3, p=1)\n",
    "    ], p=.5),\n",
    "    A.HorizontalFlip(p=.5),\n",
    "    #A.Transpose(p=.25),\n",
    "    #A.RandomRotate90(p=.25),\n",
    "    A.ShiftScaleRotate(p=.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile(f'{WRK_DIR}/mmdetection/configs/{PARAMS[\"config\"]}')\n",
    "cfg.load_from = f'{WRK_DIR}/checkpoints/{PARAMS[\"checkpoint\"]}'\n",
    "#cfg.model.roi_head.bbox_head.num_classes = 14\n",
    "cfg.model.bbox_head.num_classes = 14 # VFNet option\n",
    "#cfg.model.rpn_head.loss_bbox=dict(\n",
    "#    type='IoULoss', \n",
    "#    loss_weight=1.0)\n",
    "cfg.dump(f'{MDLS_PATH}/init_config.py')\n",
    "\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(\n",
    "        type='Resize',\n",
    "        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n",
    "                   (1333, 768), (1333, 800)],\n",
    "        multiscale_mode='value',\n",
    "        keep_ratio=True),\n",
    "    #dict(type='RandomFlip', flip_ratio=0.25),\n",
    "    ########################################\n",
    "    # Note that this key is part of bbox_params. \n",
    "    # Their difference is format='pascal_voc' means [x1, y1, x2, y2] style box encoding, \n",
    "    # while format='coco' means [x, y, w, h].\n",
    "    dict(\n",
    "        type='Albu',\n",
    "        transforms=train_transforms,\n",
    "        bbox_params=dict(\n",
    "            type='BboxParams',\n",
    "            format='pascal_voc',\n",
    "            label_fields=['gt_labels'],\n",
    "            min_visibility=0.0,\n",
    "            filter_lost_elements=True),\n",
    "        keymap={\n",
    "            'img': 'image',\n",
    "            #'gt_masks': 'masks',\n",
    "            'gt_bboxes': 'bboxes',\n",
    "        },\n",
    "        update_pad_shape=False,\n",
    "        skip_img_without_anno=True),\n",
    "    #########################################\n",
    "    dict(\n",
    "        type='Normalize',\n",
    "        mean=[103.53, 116.28, 123.675],\n",
    "        std=[1.0, 1.0, 1.0],\n",
    "        to_rgb=False),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
    "]\n",
    "\n",
    "cfg.dataset_type = 'VBDDataset'\n",
    "cfg.data_root = DATA_PATH\n",
    "cfg.data.test.type = 'VBDDataset'\n",
    "cfg.data.test.data_root = DATA_PATH\n",
    "cfg.data.test.ann_file = f'{WRK_DIR}/train.txt'\n",
    "cfg.data.test.img_prefix = ''\n",
    "cfg.data.train.type = 'VBDDataset'\n",
    "cfg.data.train.data_root = DATA_PATH\n",
    "cfg.data.train.ann_file = f'{WRK_DIR}/train.txt'\n",
    "cfg.data.train.img_prefix = ''\n",
    "cfg.data.val.type = 'VBDDataset'\n",
    "cfg.data.val.data_root = DATA_PATH\n",
    "cfg.data.val.ann_file = f'{WRK_DIR}/val.txt'\n",
    "cfg.data.val.img_prefix = ''\n",
    "cfg.work_dir = MDLS_PATH\n",
    "\n",
    "cfg.optimizer.lr = .02 / (8 * 16 / PARAMS['batch_size'])\n",
    "#cfg.optimizer = dict(type='Adam', lr=.001)\n",
    "cfg.optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
    "cfg.lr_config = dict(\n",
    "    policy='CosineAnnealing',\n",
    "    warmup='exp',\n",
    "    warmup_iters=500,\n",
    "    warmup_ratio=.1,\n",
    "    min_lr_ratio=1e-5\n",
    ")\n",
    "\n",
    "cfg.log_config.interval = 128\n",
    "cfg.runner.max_epochs = PARAMS['epochs']\n",
    "cfg.checkpoint_config.interval = 1\n",
    "cfg.evaluation = dict(interval=1, metric='mAP', save_best='mAP')\n",
    "\n",
    "cfg.seed = PARAMS['seed']\n",
    "set_random_seed(0, deterministic=False)\n",
    "\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.data.samples_per_gpu = PARAMS['batch_size']\n",
    "cfg.data.workers_per_gpu = PARAMS['batch_size']\n",
    "#cfg.workflow = [('train', 1), ('val', 1)]\n",
    "cfg.workflow = [('train', 1)]\n",
    "\n",
    "cfg.dump(f'{MDLS_PATH}/train_config.py')\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [build_dataset(cfg.data.train)]\n",
    "if len(cfg.workflow) == 2:\n",
    "    datasets.append(build_dataset(cfg.data.val))\n",
    "model = build_detector(\n",
    "    cfg.model, \n",
    "    train_cfg=cfg.get('train_cfg'), \n",
    "    test_cfg=cfg.get('test_cfg'))\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "sample = train_df.sample(n=3)['image_id'].values\n",
    "for img_id in sample:\n",
    "    boxes = train_df.loc[\n",
    "        train_df['image_id'] == img_id,\n",
    "        ['x_min', 'y_min', 'x_max', 'y_max']\n",
    "    ].values\n",
    "    img_labels = train_df.loc[\n",
    "        train_df['image_id'] == img_id, \n",
    "        ['class_id']\n",
    "    ].values.squeeze()\n",
    "    path = train_df.loc[\n",
    "        train_df['image_id'] == img_id,\n",
    "        ['img_path']\n",
    "    ].values[0][0]\n",
    "    img = cv2.imread(path)\n",
    "    for label_id, box in zip(img_labels, boxes):\n",
    "        color = label2color[label_id]\n",
    "        img = draw_bbox(\n",
    "            img, \n",
    "            list(np.int_(box)), \n",
    "            classes[label_id], \n",
    "            color\n",
    "        )\n",
    "    imgs.append(img)\n",
    "plot_imgs(imgs, size=8, cols=3, cmap=None)\n",
    "\n",
    "imgs = []\n",
    "checkpoint = f'{MDLS_PATH}/epoch_21.pth'\n",
    "cfg = f'{MDLS_PATH}/init_config.py'\n",
    "model_test = init_detector(cfg, checkpoint, device='cuda:0')\n",
    "for img_id in sample:\n",
    "    path = train_df.loc[\n",
    "        train_df['image_id'] == img_id,\n",
    "        ['img_path']\n",
    "    ].values[0][0]\n",
    "    img = mmcv.imread(path)\n",
    "    result = inference_detector(model_test, img)\n",
    "    #show_result_pyplot(model_test, img, result, score_thr=.2)\n",
    "    boxes_list = [list(x[:, :4] / PARAMS['img_size']) for x in result if x.shape[0] != 0]\n",
    "    boxes_list =  [item for sublist in boxes_list for item in sublist]\n",
    "    scores_list = [x[:, 4].tolist() for x in result if x.shape[0] != 0]\n",
    "    scores_list =  [item for sublist in scores_list for item in sublist]\n",
    "    labels_list = [[i] * x.shape[0] for i, x in enumerate(result) if x.shape[0] != 0]\n",
    "    labels_list =  [item for sublist in labels_list for item in sublist]\n",
    "    boxes, scores, box_labels = nms(\n",
    "        boxes=[boxes_list], \n",
    "        scores=[scores_list], \n",
    "        labels=[labels_list], \n",
    "        weights=None,\n",
    "        iou_thr=.5\n",
    "    )\n",
    "    boxes *= PARAMS['img_size']\n",
    "    for label_id, box, score in zip(box_labels, boxes, scores):\n",
    "        if score >= .3:\n",
    "            color = label2color[label_id]\n",
    "            img = draw_bbox(\n",
    "                img, \n",
    "                list(np.int_(box)), \n",
    "                classes[label_id], \n",
    "                color\n",
    "            )\n",
    "    imgs.append(img)\n",
    "plot_imgs(imgs, size=8, cols=3, cmap=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
